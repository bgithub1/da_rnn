{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentdex Rnn with Keras\n",
    "### This is an ipynb notebook that attempts to speed up and simply the creation of sequence data for the RNN that Harrison create in :\n",
    "#### See (https://pythonprogramming.net/crypto-rnn-model-deep-learning-python-tensorflow-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from collections import deque\n",
    "from sklearn import preprocessing\n",
    "import datetime,time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Harrison's original parameters:\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these values\n",
    "# Chose as value for SCALING_TO_USE among ['actual', 'pct', or 'minmax']\n",
    "SCALING_TO_USE = 'minmax' \n",
    "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"LTCUSD\" # change this to use different currencies as the target\n",
    "EPOCHS = 3  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "\n",
    "# DO NOT CHANGE THE VALUES BELOW\n",
    "DATA_FOLDER = './data/crypto_data'\n",
    "CURRENCY_FILE_NAMES = os.listdir(DATA_FOLDER) \n",
    "CURRENCY_LIST = [c.replace('-','').replace('.csv','')for c in CURRENCY_FILE_NAMES]\n",
    "if RATIO_TO_PREDICT not in CURRENCY_LIST:\n",
    "    raise ValueError(f'the currency {RATIO_TO_PREDICT} is not in the valid currency list {CURRENCY_LIST}')\n",
    "print(f'possible currencies to use below are: {[c.replace(\"-\",\"\").replace(\".csv\",\"\") for c in CURRENCY_FILE_NAMES]}')\n",
    "\n",
    "\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "\n",
    "print(f'using {SCALING_TO_USE} as scaling')\n",
    "print(f'using {RATIO_TO_PREDICT} currency as a target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create DataFrame of close and volume for each currency\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_future = FUTURE_PERIOD_PREDICT\n",
    "\n",
    "df_curr = None\n",
    "for i in range(len(CURRENCY_LIST)):\n",
    "    cfn = CURRENCY_FILE_NAMES[i]\n",
    "    c  = CURRENCY_LIST[i]\n",
    "    df_temp = pd.read_csv(f'{DATA_FOLDER}/{cfn}')\n",
    "    df_temp = df_temp.rename(columns={'close':f'close_{c}','volume':f'volume_{c}'})\n",
    "    df_temp.index = df_temp.timestamp\n",
    "    df_temp = df_temp[[f'close_{c}',f'volume_{c}']]\n",
    "    if i ==0:\n",
    "        df_curr = df_temp.copy()\n",
    "    else:\n",
    "        df_curr = pd.concat([df_curr,df_temp],axis=1)\n",
    "close_string = f'close_{RATIO_TO_PREDICT}'\n",
    "future_close_string = f'close_{RATIO_TO_PREDICT}_future'\n",
    "df_curr[future_close_string] = df_curr[close_string].shift(-days_in_future)\n",
    "df_curr['target'] = df_curr[future_close_string] >= df_curr[close_string]\n",
    "df_curr['hour'] = [datetime.datetime.fromtimestamp(d).hour for d  in df_curr.index]\n",
    "df_curr['minute'] = [datetime.datetime.fromtimestamp(d).minute for d  in df_curr.index]\n",
    "df_curr['year'] = [datetime.datetime.fromtimestamp(d).year for d  in df_curr.index]\n",
    "df_curr['month'] = [datetime.datetime.fromtimestamp(d).month for d  in df_curr.index]\n",
    "df_curr['day'] = [datetime.datetime.fromtimestamp(d).day for d  in df_curr.index]\n",
    "\n",
    "# get all columns except future_close_string\n",
    "cols = list(filter(lambda co: co!=future_close_string,df_curr.columns.values))\n",
    "\n",
    "\n",
    "df_curr = df_curr[(df_curr.hour>=8) & (df_curr.hour<=17)]\n",
    "df_curr['yyyymmdd'] = df_curr.year*100*100 + df_curr.month*100 + df_curr.day\n",
    "df_curr = df_curr[['yyyymmdd','hour','minute']+cols]\n",
    "\n",
    "len(df_curr),df_curr.columns.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create separate training and validation DataFrames\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all yyyymmdd's  in df_curr.yyyymmdd\n",
    "all_days = np.array(list(set(df_curr.yyyymmdd.as_matrix().reshape(-1))))\n",
    "# establish a training size percent\n",
    "train_perc = .9\n",
    "# get the index into the array all_days, which represents the last training day\n",
    "last_train_index = int(len(all_days)*train_perc)\n",
    "# get training days\n",
    "train_days = all_days[:last_train_index]\n",
    "# get df_train\n",
    "df_train = df_curr[df_curr.yyyymmdd.isin(train_days)]\n",
    "# get test days\n",
    "test_days = all_days[last_train_index:]\n",
    "# get df_test\n",
    "df_test = df_curr[df_curr.yyyymmdd.isin(test_days)]\n",
    "# print various lengths to make sure the df_train and df_test DataFrames sum to df_curr\n",
    "print(f'df_curr:{len(df_curr)}, df_train:{len(df_train)}, df_test:{len(df_test)}, should be 0: {len(df_curr)-(len(df_train)+len(df_test))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### normalize data\n",
    "For both training and validation data, provide return a dictionary with 3 DataFrames:\n",
    "1. A DataFrame of actual prices, and the actual target values\n",
    "2. A DataFrame with percentage prices, and the actual target values\n",
    "3. A DataFrame with min/max normalized percentage of the prices, and the actual (not normalized) target values.  \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df):\n",
    "    num_cols = list(filter(lambda c:'close' in c or 'volume' in c,df_train.columns.values))\n",
    "    dft = df.copy() #[['yyyymmdd','hour','minute']+num_cols]\n",
    "    dft.target = dft.target.astype(int)\n",
    "    dft = dft[~dft.isnull().any(axis=1)]\n",
    "    dft_actual = dft.copy()\n",
    "    dft[num_cols] = dft[num_cols].pct_change()\n",
    "    dft = dft[~dft.isnull().any(axis=1)]\n",
    "    dft_pct = dft.copy()\n",
    "    dft[num_cols] = (dft[num_cols] - dft[num_cols].min()) / (dft[num_cols].max() - dft[num_cols].min())\n",
    "    dft_minmax = dft.copy()\n",
    "    return {'actual':dft_actual,'pct':dft_pct,'minmax':dft_minmax}\n",
    "\n",
    "training = normalize_df(df_train)\n",
    "testing = normalize_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['actual'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['pct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['minmax'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "###  Create sequences from the minmax DataFrames\n",
    "    From the input DataFrame df, create 2 numpy arrays:\n",
    "    1. A numpy array of \"time sequences\"\n",
    "    2. A numpy array of target values, that coincide with each sequence\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(df):\n",
    "    '''\n",
    "    From the input DataFrame df, a new DataFrame with::\n",
    "    1. an 'x' column where each cell is an array of \"time sequences\" (shape of array in cell = (SEQ_LEN,num_x_cols))\n",
    "    2. a 'y' column where each cell is an array of \"time sequences\" (shape of array in cell = (SEQ_LEN,1))\n",
    "    '''\n",
    "    \n",
    "    num_cols = list(filter(lambda c: 'close' in c or 'volume' in c,df.columns.values))\n",
    "    df2 = df.copy()\n",
    "    df2['ts'] = df2.index\n",
    "    df2.index = list(range(len(df2)))\n",
    "    dfy = df2[['target']]\n",
    "    dfx = df2.drop(['target'],axis=1)\n",
    "    dx = dfx.as_matrix()\n",
    "    \n",
    "    dy = dfy.as_matrix()\n",
    "    s = len(dx)\n",
    "    sl = 60\n",
    "    last_col = len(df2.columns.values) -1\n",
    "    x_sequences = [dx[i:None if i+sl>=s else i+sl] for i in range(len(dx)-(sl-1))]\n",
    "    y_sequences = [dy[i:None if i+sl>=s else i+sl] for i in range(len(dy)-(sl-1))]\n",
    "    x_seq_dicts = [{'x':x_sequences[i]} for i in list(range(len(x_sequences)))]\n",
    "    y_seq_dicts = [{'y':y_sequences[i]} for i in list(range(len(y_sequences)))]\n",
    "    dfx = pd.DataFrame(x_seq_dicts)\n",
    "    dfy = pd.DataFrame(y_seq_dicts)\n",
    "    df_seq = dfx.join(dfy)\n",
    "    return df_seq\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "def create_daily_sequences(df):\n",
    "    '''\n",
    "    Given and input DataFrame df, run create_sequences on subsets of df \n",
    "       where each subset contains data for only a specific day.\n",
    "    Combine all of the specific \"daily\" calls to create_sequences into a single x and y array.\n",
    "    '''\n",
    "    cols_to_return = list(filter(lambda c: any([d in c for d in ['close','volume','target']]),df.columns.values))\n",
    "    df_seqs = None\n",
    "    all_days = np.array(list(set(df.yyyymmdd)))\n",
    "    for yyyymmdd in tqdm(all_days):\n",
    "        df_this_day = df[df.yyyymmdd==yyyymmdd][cols_to_return]\n",
    "        df_this_seqs = create_sequences(df_this_day)\n",
    "        if df_seqs is None:\n",
    "            df_seqs = df_this_seqs.copy()\n",
    "        else:\n",
    "            if len(df_this_seqs)>0:\n",
    "                df_seqs = df_seqs.append(df_this_seqs)\n",
    "    return df_seqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run create_daily_sequences to get final input data for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_seqs = create_daily_sequences(training[SCALING_TO_USE])\n",
    "train_x = np.array([df_train_seqs['x'].values[i] for i in range(len(df_train_seqs))])\n",
    "train_y = np.array([df_train_seqs['y'].values[i] for i in range(len(df_train_seqs))])\n",
    "\n",
    "df_test_seqs = create_daily_sequences(testing[SCALING_TO_USE])\n",
    "validation_x = np.array([df_test_seqs['x'].values[i] for i in range(len(df_test_seqs))])\n",
    "validation_y = np.array([df_test_seqs['y'].values[i] for i in range(len(df_test_seqs))])\n",
    "\n",
    "txl = train_x.shape[0]//BATCH_SIZE * BATCH_SIZE\n",
    "train_x_even = train_x[:txl]\n",
    "train_y_even = train_y[:txl]\n",
    "\n",
    "vxl = validation_x.shape[0]//BATCH_SIZE * BATCH_SIZE\n",
    "validation_x_even =  validation_x[:vxl]\n",
    "validation_y_even =  validation_y[:vxl]\n",
    "\n",
    "train_x_even.shape,train_y_even.shape,validation_x_even.shape,validation_y_even.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# TO DO, implement this\n",
    "### Balance data\n",
    "Create sequences whose corrosponding target values have an equal number of 0's and 1's\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(tseqs):\n",
    "    # get the target (y) values from the tsegs['y'] array, and put it into a DataFrame for indexing\n",
    "    dfy  = pd.DataFrame({'y':tseqs['y'].reshape(-1)})\n",
    "    # split dfy into 2 DataFrames: dfy0 contains 0's, and dfy1 contains 1's\n",
    "    dfy0 = dfy[dfy.y==0]\n",
    "    dfy1 = dfy[dfy.y==1]\n",
    "    # find the smaller DataFrame\n",
    "    dfy_len = min(len(dfy0),len(dfy1))\n",
    "    # create a new DataFrame which contains an equal number of targets with 1's and 0's\n",
    "    dfy01 = dfy0.iloc[:dfy_len].append(dfy1.iloc[:dfy_len])\n",
    "    # get the index values of this new DataFrame\n",
    "    dfy_indices= dfy01.index\n",
    "    # use those index values to locate the x sequences that corrospond to the targets in dfy01\n",
    "    tsy = tseqs['y'][dfy_indices]\n",
    "    tsx = tseqs['x'][dfy_indices]\n",
    "    # return 2 arrays: 1 for the x sequences and another for the corrosponding targets\n",
    "    return tsx,tsy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run balance_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Now create the Keras model and run it\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RNN, Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization,Flatten,SimpleRNN\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True,stateful=True,\n",
    "              batch_input_shape=(BATCH_SIZE,train_x.shape[1],train_x.shape[2])))\n",
    "# model.add(SimpleRNN(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "# # model.add(LSTM(128, return_sequences=True))\n",
    "# model.add(LSTM(128, return_sequences=True,stateful=True,\n",
    "#               batch_input_shape=(BATCH_SIZE,train_x.shape[1],train_x.shape[2])))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# # model.add(LSTM(128))\n",
    "# model.add(LSTM(128, return_sequences=True,stateful=True,\n",
    "#               batch_input_shape=(BATCH_SIZE,train_x.shape[1],train_x.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# I commented out the tensorboard stuff\n",
    "# tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "# filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "# checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x_even, train_y_even,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x_even, validation_y_even),\n",
    "#     callbacks=[tensorboard, checkpoint],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
