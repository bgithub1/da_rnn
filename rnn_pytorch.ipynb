{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## rnn_pytorch\n",
    "#### Use pytorch's nn.RNN to create predictions similar to those predicted by da_rnn.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from torch import stack as tsk #@UnresolvedImport\n",
    "from torch import optim\n",
    "from torch import from_numpy #@UnresolvedImport\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Define pytorch Module\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size=32):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=hidden_size,     # rnn hidden unit\n",
    "            num_layers=1,       # number of rnn layer\n",
    "            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x_in, h_state):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, hidden_size)\n",
    "        h = None if h_state is None else from_numpy(h_state.numpy().astype(np.float32))\n",
    "        x = from_numpy(x_in.numpy().astype(np.float32))\n",
    "        r_out, h_state = self.rnn(x, h)\n",
    "\n",
    "        outs = []    # save all predictions\n",
    "        for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "#         return torch.stack(outs, dim=1), h_state\n",
    "        return tsk(outs, dim=1), h_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Define model runner\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnPytorch():\n",
    "    def __init__(self,\n",
    "        df=None,\n",
    "        reuse_indices_in_batches = True,\n",
    "        epochs = 400,\n",
    "        batch_size = 1, \n",
    "        time_step = 10,\n",
    "        y_col = 'y',\n",
    "        index_col = 'pi',\n",
    "        test_percentage = .2,\n",
    "        learning_rate = .02):\n",
    "        \n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.time_step = time_step\n",
    "        self.y_col = y_col\n",
    "        self.index_col = index_col\n",
    "        self.test_percentage = test_percentage\n",
    "\n",
    "        self.df = self.default_data() if df is None else df.copy()\n",
    "        x_cols = list(filter(lambda c: c!=y_col,self.df.columns.values))\n",
    "        if self.index_col is not None:\n",
    "            x_cols = list(filter(lambda c: c!=self.index_col, x_cols))\n",
    "        self.x_cols = x_cols\n",
    "        \n",
    "        # create member variables\n",
    "        self.rnn = SimpleRNN(len(self.x_cols))\n",
    "        self.optimizer = optim.Adam(self.rnn.parameters(), lr=learning_rate)   # optimize all cnn parameters\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.test_size = len(self.df) - int(len(self.df) * (1-test_percentage))\n",
    "        train_len = int(len(self.df) * (1-self.test_percentage))\n",
    "        df_train_data = self.df.iloc[:train_len]\n",
    "        \n",
    "        self.training_batches = self.create_batches(df_train_data,reuse_indices_in_batches=reuse_indices_in_batches)\n",
    "        dt_test_data = self.df[-self.test_size:]\n",
    "        self.test_batches = self.create_test_data(dt_test_data)\n",
    "        self.df_test = self.test_batches['df_test']\n",
    "        pass\n",
    "        \n",
    "    def create_batches(self,df_train_data,reuse_indices_in_batches=True):\n",
    "        # determine the label column\n",
    "        lc = self.y_col\n",
    "        # determine the columns that are features (x columns) and the column that will have index values\n",
    "        indices = np.array(df_train_data.index)\n",
    "        x_cols = self.x_cols \n",
    "        if self.index_col is not None:\n",
    "#             x_cols = list(filter(lambda c: c!=self.index_col, x_cols))\n",
    "            indices = np.array(df_train_data[self.index_col])\n",
    "        indices = indices.reshape(-1,1)\n",
    "        df_x = df_train_data[x_cols]\n",
    "        df_y = df_train_data[[lc]]\n",
    "        # get x and y\n",
    "        x = df_x.as_matrix()\n",
    "        y = df_y.as_matrix()\n",
    "        # now make sure that the length of x is divisible by batch_size * time_step * x_cols\n",
    "        lenx = x.shape[0]\n",
    "        lenx = (lenx // (self.batch_size * self.time_step)) * (self.batch_size * self.time_step)\n",
    "        \n",
    "        x = x[:lenx]\n",
    "        y = y[:lenx]\n",
    "        indices = indices[:lenx]\n",
    "        \n",
    "        if reuse_indices_in_batches:\n",
    "            tsgl = len(indices) -  self.time_step * self.batch_size # time step group length\n",
    "            random_indices = np.random.permutation(tsgl) \n",
    "            x_time_step_groups = np.array([np.array(x[i:i+self.time_step]) for i in range(tsgl)])[random_indices]\n",
    "            x_batches = np.array(x_time_step_groups).reshape(-1,self.batch_size,x_time_step_groups.shape[1],x_time_step_groups.shape[2])\n",
    "            \n",
    "            y_time_step_groups = np.array([y[i:i+self.time_step] for i in range(tsgl)])[random_indices]\n",
    "            y_batches = np.array(y_time_step_groups).reshape(-1,self.batch_size,y_time_step_groups.shape[1],y_time_step_groups.shape[2])\n",
    "            \n",
    "            i_time_step_groups = np.array([indices[i:i+self.time_step] for i in range(tsgl)])[random_indices]\n",
    "            i_batches = np.array(i_time_step_groups).reshape(-1,self.batch_size,i_time_step_groups.shape[1],i_time_step_groups.shape[2])\n",
    "    \n",
    "        else:\n",
    "            x_batches = x.reshape(-1,self.batch_size,self.time_step,len(x_cols))\n",
    "            y_batches = y.reshape(-1,self.batch_size,self.time_step,1)\n",
    "            i_batches = indices.reshape(-1,self.batch_size,self.time_step,1)\n",
    "        training_batches = {'i':i_batches,'x':x_batches,'y':y_batches}   \n",
    "        \n",
    "        return training_batches\n",
    "\n",
    "    def create_test_data(self,df_test_data):\n",
    "        \n",
    "        # create test data to be used when plotting progress\n",
    "        test_batches = self.create_batches(df_test_data,reuse_indices_in_batches=False)\n",
    "        \n",
    "        # create input data for creating predictions that you can plot during training\n",
    "        i_batches_test = test_batches['i']\n",
    "        y_actual_test = test_batches['y']\n",
    "        \n",
    "        # reshape your prediction batches data so that they 1 dimensional arrays that can go into the columns of a DataFrame\n",
    "        i_test = i_batches_test.reshape(-1)\n",
    "        y_test = y_actual_test.reshape(-1)\n",
    "        \n",
    "        # create DataFrame that is used for plotting during training to see progress\n",
    "        df_test = pd.DataFrame({'y_actual':y_test,'y_pred':np.zeros(len(y_test))})\n",
    "        # reset that DataFrame's index \n",
    "        df_test.index = i_test\n",
    "        test_batches['df_test'] = df_test\n",
    "        return test_batches\n",
    "\n",
    "    def plot_it(self,executed_first_plot,h_state):\n",
    "        rnn_validator = self.rnn.eval()\n",
    "        x_batches_test = self.test_batches['x']\n",
    "        i_batches_test = self.test_batches['i']\n",
    "        # use first dimension of x_batches to get the total number of batches\n",
    "        plot_steps = x_batches_test.shape[0]\n",
    "        h_state_temp = h_state\n",
    "        for plot_step in range(plot_steps):\n",
    "            x_batch_test = x_batches_test[plot_step]\n",
    "            x_batch_test_tensor = from_numpy(x_batch_test)    # shape (batch, time_step, input_size)\n",
    "            test_predict,h_state_temp = rnn_validator(x_batch_test_tensor,h_state_temp)\n",
    "            h_state_temp = h_state_temp.data\n",
    "            y_predictions = test_predict.data.numpy().reshape(-1)\n",
    "            indices_for_y_predictions = i_batches_test[plot_step].reshape(-1) \n",
    "            self.df_test.loc[indices_for_y_predictions,'y_pred'] = y_predictions\n",
    "        df_display = self.df_test.iloc[-100:]\n",
    "        if not executed_first_plot:\n",
    "            self.line1, = self.ax.plot(df_display.index, df_display.y_actual, 'r-')\n",
    "            self.line2, = self.ax.plot(df_display.index, df_display.y_pred, 'b-')\n",
    "            executed_first_plot = True\n",
    "        else:\n",
    "            self.line1.set_ydata(df_display.y_actual)\n",
    "            self.line2.set_ydata(df_display.y_pred)\n",
    "            self.fig.canvas.start_event_loop(0.05)\n",
    "\n",
    "    def default_data(self):\n",
    "        time_steps = self.time_step\n",
    "        epochs = self.epochs\n",
    "        # show data\n",
    "        steps = np.linspace(0, np.pi*epochs, epochs*time_steps, dtype=np.float32)  # float32 for converting torch FloatTensor\n",
    "        x_np = np.sin(steps)\n",
    "        y_np = np.cos(steps)\n",
    "        dff = pd.DataFrame({'pi':steps,'x':x_np,'y':y_np})\n",
    "        return dff\n",
    "\n",
    "    def run(self):\n",
    "        plt.figure(1, figsize=(12, 5))\n",
    "        plt.ion()           # continuously plot\n",
    "        steps = self.training_batches['i'].shape[0] \n",
    "        self.fig = plt.figure()\n",
    "        self.ax = plt.gca()\n",
    "        executed_first_plot=False\n",
    "        \n",
    "        h_state = None      # for initial hidden state\n",
    "        for step in range(steps):\n",
    "            \n",
    "    #         i_np,x_np,y_np = create_mini_batches(batches, [step])\n",
    "            i_np,x_np,y_np = self.training_batches['i'][step],self.training_batches['x'][step],self.training_batches['y'][step]\n",
    "            \n",
    "            x = torch.Tensor(x_np)    # shape (batch, time_step, input_size)\n",
    "            y = torch.Tensor(y_np)\n",
    "        \n",
    "            prediction, h_state = self.rnn(x, h_state)   # rnn output\n",
    "            # !! next step is important !!\n",
    "            h_state = h_state.data        # repack the hidden state, break the connection from last iteration\n",
    "        \n",
    "            loss = self.loss_func(prediction, y)         # calculate loss\n",
    "            if float(loss.data[0]<.00002):\n",
    "                print(f'plotting step {step}.  loss = {loss.data[0]}')\n",
    "                self.plot_it(executed_first_plot,h_state)\n",
    "                executed_first_plot = True\n",
    "                break\n",
    "            self.optimizer.zero_grad()                   # clear gradients for this training step\n",
    "            loss.backward()                         # backpropagation, compute gradients\n",
    "            self.optimizer.step()                        # apply gradients\n",
    "    \n",
    "            if step % 20 == 0: \n",
    "                print(f'plotting step {step}.  loss = {loss.data[0]}')\n",
    "                self.plot_it(executed_first_plot,h_state)\n",
    "                executed_first_plot = True\n",
    "                    \n",
    "        print('done')\n",
    "        plt.ioff()\n",
    "        input('hit return to exit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Define main method\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bperlman1/Virtualenvs3/pyliverisk/lib/python3.6/site-packages/ipykernel_launcher.py:164: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/bperlman1/Virtualenvs3/pyliverisk/lib/python3.6/site-packages/ipykernel_launcher.py:174: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting step 0.  loss = 131.96463012695312\n",
      "plotting step 20.  loss = 0.3230140209197998\n",
      "plotting step 40.  loss = 1.608119010925293\n",
      "plotting step 60.  loss = 0.40139949321746826\n",
      "plotting step 80.  loss = 0.3564144968986511\n",
      "plotting step 100.  loss = 0.19318625330924988\n",
      "plotting step 120.  loss = 0.2653404772281647\n",
      "plotting step 140.  loss = 0.23433224856853485\n",
      "plotting step 160.  loss = 0.2288234531879425\n",
      "plotting step 180.  loss = 0.17670859396457672\n",
      "plotting step 200.  loss = 0.16198867559432983\n",
      "plotting step 220.  loss = 0.2516464293003082\n",
      "plotting step 240.  loss = 0.08391830325126648\n",
      "plotting step 260.  loss = 0.24470007419586182\n",
      "plotting step 280.  loss = 0.5800777673721313\n",
      "plotting step 300.  loss = 0.48953744769096375\n",
      "plotting step 320.  loss = 0.25039735436439514\n",
      "plotting step 340.  loss = 0.27197763323783875\n",
      "plotting step 360.  loss = 0.28841957449913025\n",
      "plotting step 380.  loss = 0.17249561846256256\n",
      "plotting step 400.  loss = 0.21822935342788696\n",
      "plotting step 420.  loss = 0.14551986753940582\n",
      "plotting step 440.  loss = 0.41292089223861694\n",
      "plotting step 460.  loss = 0.3477323651313782\n",
      "plotting step 480.  loss = 0.31581372022628784\n",
      "plotting step 500.  loss = 0.27996698021888733\n",
      "plotting step 520.  loss = 0.10119032114744186\n",
      "done\n",
      "hit return to exit\n",
      "done done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    df_uso = pd.read_csv('./data/uso_201812.csv')\n",
    "    #close    high    low    open    symbol    timestamp    tradingDay    volume\n",
    "    df_uso = df_uso[['open','high','low','close','volume','tradingDay']]\n",
    "    df_uso['year'] = df_uso.tradingDay.apply(lambda d:int(str(d)[0:4]))\n",
    "    df_uso['month'] = df_uso.tradingDay.apply(lambda d:int(str(d)[5:7]))\n",
    "    df_uso['day'] = df_uso.tradingDay.apply(lambda d:int(str(d)[8:10]))\n",
    "    df_uso = df_uso[['open','high','low','close','volume','year','month','day']]\n",
    "    df_uso['ic'] = df_uso.index\n",
    "    df_uso['y'] = df_uso.iloc[1:].close\n",
    "    df_uso = df_uso[1:]\n",
    "    rnnpy = RnnPytorch(df = df_uso,index_col='ic',\n",
    "                       reuse_indices_in_batches=True,epochs = 100000,batch_size=10)\n",
    "#     rnnpy.df.to_csv('./df_rnn_pytorch.csv',index=False)\n",
    "    rnnpy.run()\n",
    "\n",
    "    print('done done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
